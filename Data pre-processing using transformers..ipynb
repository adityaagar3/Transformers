{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541ab1bc",
   "metadata": {},
   "source": [
    "While using a pretrained model, use the associated pretrained tokenizer: it will split the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence token to index (that we usually call a vocab) as during pretraining.\n",
    "\n",
    "### To automatically download the vocab used during pretraining or fine-tuning a given model, you can use the from_pretrained() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb012a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e0e8955bb545c081b94693deca4939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a335cf57ea746eda85bbc9d8701cea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eecf05a6dfc4f27b499f59b7512c4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91d36d5a2934a05b8c98f023c09d683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d3f49",
   "metadata": {},
   "source": [
    "### tokenizer is a pipeline that do\n",
    "### Rawtext ---> Token -----> Special Tokens -----> Inputids for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac73d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', \"'\", 'm', 'a', 'single', 'sentence', '!']\n"
     ]
    }
   ],
   "source": [
    "## Rawtext to Tokens\n",
    "tokens = tokenizer.tokenize(\"Hello, I'm a single sentence!\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036156a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8667, 117, 146, 112, 182, 170, 1423, 5650, 106]\n"
     ]
    }
   ],
   "source": [
    "## Rawtext to Inputids\n",
    "inputids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(inputids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e954e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 112, 182, 170, 1423, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "## Inputids to Special Tokens and final inputs \n",
    "fnalinputids = tokenizer.prepare_for_model(inputids)\n",
    "print(fnalinputids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15fcb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Hello, I'm a single sentence! [SEP]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(fnalinputids['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f3e1b",
   "metadata": {},
   "source": [
    "### With multiple sentences : to be processed in batch.\n",
    "It will return dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7e857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                       \"And another sentence\",\n",
    "                       \"And the very very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7958f3",
   "metadata": {},
   "source": [
    "### Padding and truncation to build a model. Instead of list getting tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b79a46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102]])>, 'token_type_ids': <tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\",max_length=8)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d0e23",
   "metadata": {},
   "source": [
    "### Check. for 1st sentence. truncation happens. And for 2nd sentence padding happened\n",
    "Also, attention_mask points out which tokens the model should pay attention to and which ones it should not (because they represent padding in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "047ff450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, 1201, 1385, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a70745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 21), dtype=int32, numpy=\n",
      "array([[  101,  8667,   146,   112,   182,   170,  1423,  5650,   102,\n",
      "          146,   112,   182,   170,  5650,  1115,  2947,  1114,  1103,\n",
      "         1148,  5650,   102],\n",
      "       [  101,  1262,  1330,  5650,   102,  1262,   146,  1431,  1129,\n",
      "        12544,  1114,  1103,  1248,  5650,   102,     0,     0,     0,\n",
      "            0,     0,     0],\n",
      "       [  101,  1262,  1103,  1304,  1304,  1314,  1141,   102,  1262,\n",
      "          146,  1301,  1114,  1103,  1304,  1314,  1141,   102,     0,\n",
      "            0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(3, 21), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(3, 21), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\", \"And another sentence\", \"And the very very last one\"]\n",
    "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
    "                             \"And I should be encoded with the second sentence\",\n",
    "                             \"And I go with the very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences,padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bf429",
   "metadata": {},
   "source": [
    "### Here you can see token_type_ids is 0 for 1st sentence tokens and 1 for 2nd sentence token. \n",
    "Not every model require token_type_ids, you can force it by using return_input_ids or return_token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
